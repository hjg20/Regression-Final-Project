---
title: "Final Project"
author: "Hunter Garrison, Kevin Smith, Reese Madsen, Giulio Martini"
date: "2024-04-26"
output:
  pdf_document: default
  html_document:
    df_print: paged
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Section 1: Introduction

In this project, we are working with a dataset comprising of the observed prices of houses and corresponding attributes of each house. This dataset is from the database website Kaggle and is comprised of 545 observations of houses in India with 13 variables recorded for each specific house. Below are the attributes of each house recorded in the dataset:

| Attribute               | Description                                                             | Data Type |
|------------------|-------------------------------------|------------------|
| [**price**]{.underline} | Price of the houses in rupees. (this is our response variable)          | Integer   |
| area                    | Area of a house in square feet                                          | Integer   |
| bedrooms                | Number of bedrooms in house                                             | Integer   |
| bathrooms               | Number of bathrooms in house                                            | Integer   |
| stories                 | Number of stories in house                                              | Integer   |
| mainroad                | Whether or not house is connected to main road                          | Boolean   |
| guestroom               | Whether or not house has a guest room                                   | Boolean   |
| basement                | Whether or not house has a basement                                     | Boolean   |
| hotwaterheating         | Whether or not house has a hot-water heater                             | Boolean   |
| airconditioning         | Whether or not house has air conditioning                               | Boolean   |
| parking                 | Number of parking spots at house                                        | Integer   |
| prefarea                | Whether or not the house is in a preferred area                         | Boolean   |
| furnishingstatus        | Furnishing status of the house (furnished, semi-furnished, unfurnished) | String    |

The majority of our predictors in this dataset are of type boolean, with the remainders being integers and one categorical predictor comprising of strings. Below is the first 5 lines of our dataset:

```{r, echo=FALSE}
data <- read.csv('Housing.csv')
head(data, 5)
```

We are attempting to create a linear regression model that best explains the variance in the price of the houses using the predictors in our dataset. In this report, we will explore our data by finding collinearity, outliers, influential points, and by checking error assumption violations. We will also discover which variables are the most important by using model selection, and test OLS as well as GLS and WLS regressions. After doing this, we will combine all the knowledge we have gained about our data set in order to find the model in which we believe best explains the variance in the price of the houses.

We will now perform some exploratory data analysis to show the distribution of our response variable as well as some key relationships in this dataset. Below is a histogram of our response variable, price, as well as a table showing some key statistical information about the response:

```{r, include=FALSE}
library(dplyr)
```

```{r, echo=FALSE}
hist(data$price, xlab="Price of House", ylab="Frequency", main='Histogram of Price' ,col="purple")

price_summary <- data %>%
  summarise(
    Min = min(price, na.rm = TRUE),
    Q1 = quantile(price, 0.25, na.rm = TRUE),
    Median = median(price, na.rm = TRUE),
    Mean = mean(price, na.rm = TRUE),
    Q3 = quantile(price, 0.75, na.rm = TRUE),
    Max = max(price, na.rm = TRUE),
    NA_Count = sum(is.na(price))
  )

print(price_summary)
```

From our histogram and table, we see that our data is right skewed with a small amount of houses showing prices much farther than the mean. These houses may get removed as outliers later on in our report. Below, we also show a scatter plot of price vs. area:

```{r, echo=FALSE}
plot(data$area, data$price, xlab='Area', ylab='Price', main='Price vs. Area', col='blue')
```

We can see a visible linear correlation between the two variables, suggesting that price will be a large factor in the models we will test.

All in all, this dataset is very interesting to us because of how it can teach us the relationships between a house's price and attributes about the house and can help us better understand the housing market when we need to go out and purchase a house for ourselves.

# Section 2: Regression Analysis

### Collinearity

In our study of the dataset we must ask ourselves whether or not the data is plagued by the issue of collinearity. Do the predictors present a linear relationship, not just with the response, but within each other as well? Collinearity can hurt a model's performance; inflated R\^2 scores and diminished p-values are but a few of the problems that arise alongside predictor collinearity... and so, the dataset must be tested for it.

Let us begin by creating a ***correlation heat-map*** for each predictor. This measures the correlation between predictors, and provides us with a visual method of estimating it. Each categorical column is *label encoded* - their class value is turned to a constant integer - and is added to the correlation matrix.

```{r, include=FALSE}
library(dplyr)
library(corrplot)
library(forcats)
```

```{r, include=FALSE}
data <- data %>%
  mutate(across(where(is.character), as.factor))

data_encoded <- data %>%
  mutate(across(where(~is.factor(.) && nlevels(.) <= 3), as.integer))

data_numeric_only <- data_encoded %>%
  select(where(is.numeric))

housing_preds <- select(data_numeric_only, -price)

cor_matrix <- cor(housing_preds, use = "complete.obs")
rounded_cor_matrix <- round(cor_matrix, 3)

print(rounded_cor_matrix)
```

```{r, echo=FALSE}
cor_matrix <- cor(housing_preds, use = "complete.obs")

# Create the correlation plot
corrplot(cor_matrix, method = 'color', order = 'hclust', diag = FALSE,
         number.cex = 0.7, addCoef.col = 'black', tl.pos = 'd', tl.cex = 0.6, cl.cex = 0.7, cl.pos = 'r')
```

As we can see by the correlation heat-map, correlations between each predictor are realtively small. The strongest correlation is between *stories* and *bedrooms*, with a value of 0.41, swiftly followed by *bedrooms* and *bathrooms* with a coefficient of 0.37, and by *parking* and *area*, with a value of 0.29. These coefficients are decently small, however we will proceed with a more thorough investigation to make sure collinearity is not one of this dataset's problems.

```{r, include=FALSE}
pairs(housing_preds, col = 'dodgerblue', pch=1)
```

```{r, include=FALSE}
library(olsrr)
```

```{r, echo=FALSE}
model = lm(price ~ ., data = data_numeric_only)

round(ols_eigen_cindex(model)[, 1:2], 4)
```

The next step is viewing the dataset's condition numbers, based on a linear model's (that is using the dataset) values.

| Eigenvalue | Condition Index |
|:-----------|:----------------|
| 11.4591    | 1.0000          |
| 0.5846     | 4.4275          |
| 0.2140     | 7.3182          |
| 0.1537     | 8.6333          |
| 0.1229     | 9.6550          |
| 0.1016     | 10.6223         |
| 0.0886     | 11.3721         |
| 0.0778     | 12.1358         |
| 0.0622     | 13.5738         |
| 0.0569     | 14.1939         |
| 0.0403     | 16.8673         |
| 0.0309     | 19.2715         |
| 0.0075     | 38.9969         |

The condition number is 38.9969, and usually a value above 30 marks the dataset for collinearity. This value of condition number suggests that collinearity should be a problem in this dataset. To further confirm this hypothesis, we shall check each predictor's VIF value.

```{r, include=FALSE}
library(faraway)
```

```{r, include=FALSE}
vif(model)
```

| Variable         | VIF      |
|:-----------------|:---------|
| area             | 1.325208 |
| bedrooms         | 1.367503 |
| bathrooms        | 1.286559 |
| stories          | 1.478029 |
| mainroad         | 1.172661 |
| guestroom        | 1.212687 |
| basement         | 1.320749 |
| hotwaterheating  | 1.039293 |
| airconditioning  | 1.207262 |
| parking          | 1.211959 |
| prefarea         | 1.148598 |
| furnishingstatus | 1.095641 |

Only predictors with VIFs above 5 are considered problematic, however we can see that, in this dataset, the highest VIF value is 1.478029. We could stop, however we wish to make the investigation very thorough.

We can then check if the variables are orthogonal. An orthogonal variable is not plagued by multicollinearity, and has a R\^2_k of less than 0.3. Below is a table of these values for each predictor

```{r, include=FALSE}
1 - 1/vif(model)
```

| Variable         | $R^2_k$    |
|:-----------------|:-----------|
| area             | 0.24540168 |
| bedrooms         | 0.26874003 |
| bathrooms        | 0.2273304  |
| stories          | 0.32342322 |
| mainroad         | 0.14723849 |
| guestroom        | 0.17538517 |
| basement         | 0.24285414 |
| hotwaterheating  | 0.03780781 |
| airconditioning  | 0.17167947 |
| parking          | 0.17488936 |
| prefarea         | 0.12937340 |
| furnishingstatus | 0.08729247 |

We can see that most predictors are orthogonal, with the exception of *stories* which has a value of 0.3234.

If, perchance, there was an issue of collinearity, we could check the individual condition numbers. They would have to be above 30. Each value above 30 would signify a singular instance of a problematic linear dependence between predictors. Luckily, there is none here, as you can see from the values below.

```{r, echo=FALSE}
round(ols_eigen_cindex(model), 3)
```

With all the information provided, we then conclude, with confidence, that multicollinearity is not a problem of this dataset.

### Variable Selection

We now move on to selecting variables to use in our final model. We will this using forward, backward, and step-wise selection using AIC, BIC, and RMSE LOOCV to determine which variables are the best for our model

```{r}
n = nrow(data)
mod_all_preds = lm(price~., data=data)
mod_back_aic = step(mod_all_preds, direction='backward')
```

```{r}
n = nrow(data)
mod_all_preds = lm(price~., data=data)
mod_back_bic = step(mod_all_preds, direction='backward', k=log(n))
```

```{r}
n = nrow(data)
mod_start = lm(price~1, data=data)
mod_forwd_aic = step(mod_start, scope=price~mainroad+area+parking+prefarea+guestroom+basement+airconditioning+bathrooms+bedrooms+stories+hotwaterheating+furnishingstatus, direction='forward')
```

```{r}
n = nrow(data)
mod_start = lm(price~1, data=data)
mod_forwd_bic = step(mod_start, scope=price~mainroad+area+parking+prefarea+guestroom+basement+airconditioning+bathrooms+bedrooms+stories+hotwaterheating+furnishingstatus, direction='forward', k=log(n))
```

```{r}
n = nrow(data)
mod_start = lm(price~1, data=data)
mod_step_aic = step(mod_start, scope=price~mainroad+area+parking+prefarea+guestroom+basement+airconditioning+bathrooms+bedrooms+stories+hotwaterheating+furnishingstatus, direction='both')
```

```{r}
n = nrow(data)
mod_start = lm(price~1, data=data)
mod_step_bic = step(mod_start, scope=price~mainroad+area+parking+prefarea+guestroom+basement+airconditioning+bathrooms+bedrooms+stories+hotwaterheating+furnishingstatus, direction='both', k=log(n))
```

```{r}
calc_loocv_rmse = function(model) {
  sqrt(mean((resid(model) / (1 - hatvalues(model))) ^ 2))
}
calc_loocv_rmse(mod_back_aic)
calc_loocv_rmse(mod_back_bic)
calc_loocv_rmse(mod_forwd_aic)
calc_loocv_rmse(mod_forwd_bic)
calc_loocv_rmse(mod_step_aic)
calc_loocv_rmse(mod_step_bic)
```

```{r}
library(leaps)

mod_exhaustive = summary(regsubsets(price ~ ., data = data, nvmax = 8))
```

```{r}
best_r2_ind = which.max(mod_exhaustive$adjr2)

mod_exhaustive$which[best_r2_ind,]
mod_exhaust_adjr2 <- lm(price~area+bathrooms+stories+basement+airconditioning+parking+prefarea+furnishingstatus, data=data)
summary(mod_exhaust_adjr2)
```

### Model Diagnostics
